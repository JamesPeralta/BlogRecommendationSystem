{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data is broken down into 5 main components\n",
    "- [Data Cleaning](#Data-Cleaning)\n",
    "- [Exploratory Analysis](#Exploratory-Analysis)\n",
    "- [Generating training/validation/test sets](#Generating-Train/Validation/Test-sets)\n",
    "- [Training Model](#Training-Models)\n",
    "- [Evaluating Model](#Evaluating-Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=’Cleaning’></a>\n",
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "shared_articles = pd.read_csv('shared_articles.csv')\n",
    "user_interactions = pd.read_csv('users_interactions.csv')\n",
    "\n",
    "# Only care about when articles were posted and english ones for now\n",
    "shared_articles = shared_articles.loc[shared_articles[\"eventType\"] == \"CONTENT SHARED\"]\n",
    "shared_articles = shared_articles.loc[shared_articles[\"lang\"] == \"en\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import re, string\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    text: str, returns: str\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = ' '.join([stemmer.stem(w) for w in text.split() if w not in ENGLISH_STOP_WORDS])\n",
    "    return text\n",
    "\n",
    "def parse_url(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "shared_articles['url'] = shared_articles['url'].apply(parse_url)\n",
    "shared_articles['authorPersonId'] = shared_articles['authorPersonId'].apply(str)\n",
    "shared_articles['text'] = shared_articles['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Virality Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_map = {\n",
    "        \"VIEW\": 1,\n",
    "        \"LIKE\": 4,\n",
    "        \"COMMENT CREATED\": 10,\n",
    "        \"FOLLOW\": 25,\n",
    "        \"BOOKMARK\": 100\n",
    "}\n",
    "\n",
    "def generate_virality_score(row):    \n",
    "    event_type = row[\"eventType\"]\n",
    "    row[\"score\"] = score_map[event_type]\n",
    "    \n",
    "    return row\n",
    "\n",
    "user_interactions = user_interactions.apply(generate_virality_score, axis=1)\n",
    "aggregated_interactions = user_interactions.groupby(['contentId'])[['score']].sum()\n",
    "aggregated_interactions = aggregated_interactions.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left outer join to maintain posts with no score\n",
    "joined_dataset = shared_articles.merge(aggregated_interactions, 'left', on='contentId').sort_values('score', ascending=False)\n",
    "\n",
    "# Post with no interactions are set to 0\n",
    "joined_dataset['score'] = joined_dataset['score'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=’Explore’></a>\n",
    "# Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I explore some of the user interactions. Some interesting statistics are\n",
    "  - 9.4% of views converts into a like\n",
    "  - 4% of views converts into a bookmark\n",
    "  - 2.6% of views converts into a comment\n",
    "  - 2.3% of views converts into a follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_interactions[['eventType']].value_counts())\n",
    "user_interactions[['eventType']].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the two tables after joining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the first look after joining the two tables, the data is very skewed which makes this problem very hard to model using a regression technique. After further examination, we see that 90% of posts have a score under 324 and 99% of posts have a score under 1039. This makes me think that I should probably use a classification technique because the data is too skewed for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(joined_dataset['score'].quantile([.1, .2, .5, .8, .9, 0.95, 0.99]))\n",
    "joined_dataset['score'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I attempt to analyze the number of unique values in each categorical column for one hot encoding purposes. I see that there are 754 unique urls and 187 unique authors so one hot encoding will blow up the feature space. Instead I will probably one hot encode the top 50 urls and top 50 authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(joined_dataset.nunique())\n",
    "joined_dataset[[ 'url', 'authorPersonId', 'authorUserAgent', 'authorRegion', 'authorCountry']].nunique().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I now view number of nulls in each feature. We see that ~79% of rows have nans for authorUserAgent, authorRegion, and authorCountry. I will drop these features because they have too many nans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Total rows: \", len(joined_dataset.index))\n",
    "joined_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_dataset = joined_dataset.drop(['authorUserAgent', 'authorRegion', 'authorCountry'], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=’GenerateSets’></a>\n",
    "# Generating Train/Validation/Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I will now generate the labels. I saw from exploration that predicting the virality index value for a given post will be very hard because the dataset is very skewed. Instead I will turn this into a classification problem where a post is either viral or non-viral. Viral posts are posts that acheive a score over 500. Non viral posts are any post less than 500. This threshold was choosen by looking at the quantiles which tells us only 5% of posts get over a score of 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label(row):  \n",
    "    if row.score < 500:\n",
    "        row[\"label\"] = \"Not Viral\"\n",
    "    else:\n",
    "        row[\"label\"] = \"Viral\"\n",
    "    \n",
    "    return row\n",
    "\n",
    "joined_dataset = joined_dataset.apply(generate_label, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into 70/15/15 split for train/validation/test sets respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "features = joined_dataset.drop(['score', 'label'], axis='columns')\n",
    "labels = joined_dataset['label']\n",
    "\n",
    "print(f\"Total features: {len(features)} rows, {len(features.columns)} columns\")\n",
    "print(f\"Total labels: {len(labels)} rows\")\n",
    "\n",
    "# Intermediate rows which will be divided further into Test/Train later using different random states\n",
    "X_intermediate, X_test, y_intermediate, y_test = train_test_split(features, \n",
    "                                                                  labels, \n",
    "                                                                  test_size=0.15, \n",
    "                                                                  random_state=0,\n",
    "                                                                  stratify=labels)\n",
    "\n",
    "# Intermediate rows are divided further into Train and Validation\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_intermediate, \n",
    "                                                                y_intermediate, \n",
    "                                                                test_size=0.17, \n",
    "                                                                random_state=0,\n",
    "                                                                stratify=y_intermediate)\n",
    "\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Validation size: {len(X_validation)}\")\n",
    "print(f\"Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate top 50 urls and top 50 authors for one hot encoiding by only using what I know from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "\n",
    "# Generate Categorical features\n",
    "top_50_urls = [x for x in X_train.url.value_counts().sort_values(ascending=False).head(50).index]\n",
    "top_50_authors = [x for x in X_train.authorPersonId.value_counts().sort_values(ascending=False).head(50).index]\n",
    "\n",
    "for label in top_50_urls:\n",
    "    X_train[label] = np.where(X_train['url'] == label, 1, 0)\n",
    "    X_validation[label] = np.where(X_validation['url'] == label, 1, 0)\n",
    "    X_test[label] = np.where(X_test['url'] == label, 1, 0)\n",
    "\n",
    "for label in top_50_authors:\n",
    "    X_train[label] = np.where(X_train['authorPersonId'] == label, 1, 0)\n",
    "    X_validation[label] = np.where(X_validation['authorPersonId'] == label, 1, 0)\n",
    "    X_test[label] = np.where(X_test['authorPersonId'] == label, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a TF-IDF vectorizer on the training set text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2, max_df=.5, max_features=1000)\n",
    "vectorizer.fit(X_train[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate features as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_feature_arr(feature_df, vectorizer):\n",
    "    # Generate features as NumPy arrays\n",
    "    text_arr = vectorizer.transform(feature_df[\"text\"]).toarray()\n",
    "    url_arr = feature_df[[label for label in top_50_urls]].to_numpy()\n",
    "    author_arr = feature_df[[label for label in top_50_authors]].to_numpy()\n",
    "    \n",
    "    final_arr = np.append(text_arr, url_arr, axis=1)\n",
    "    final_arr = np.append(final_arr, author_arr, axis=1)\n",
    "    return final_arr\n",
    "\n",
    "X_train = generate_feature_arr(X_train, vectorizer)\n",
    "X_validation = generate_feature_arr(X_validation, vectorizer)\n",
    "X_test = generate_feature_arr(X_test, vectorizer)\n",
    "\n",
    "print(\"Train feature shape: \", X_train.shape)\n",
    "print(\"Validation feature shape: \", X_validation.shape)\n",
    "print(\"Test feature shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=’Train’></a>\n",
    "# Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "def custom_predict(classifier, data_points):\n",
    "    y_pred = (classifier.predict_proba(data_points)[:,1] >= 0.055)\n",
    "    predictions = [\"Viral\" if pred else \"Not Viral\" for pred in y_pred]\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def compute_metrics(classifier, model_name):\n",
    "    predictions = custom_predict(classifier, X_validation)\n",
    "    \n",
    "    acc = accuracy_score(y_validation, predictions)\n",
    "    prec = precision_score(y_validation, predictions, pos_label='Viral')\n",
    "    rec = recall_score(y_validation, predictions, pos_label='Viral')\n",
    "    \n",
    "    print(f\"{model_name} scores: \\n  Accuracy: {round(acc * 100, 2)}%\\n  Precision: {round(prec * 100, 2)}%\\n  Recall: {round(rec * 100, 2)}%\")\n",
    "\n",
    "# LogisticRegression\n",
    "lr = LogisticRegression(random_state=1, solver='lbfgs').fit(X_train, y_train)\n",
    "compute_metrics(lr, \"Logistic Regression\")\n",
    "\n",
    "# Naive Bayes, with Bernoulli distribution\n",
    "nv_bd = BernoulliNB().fit(X_train, y_train)\n",
    "compute_metrics(nv_bd, \"Naive Bayes\")\n",
    "\n",
    "# Decision tree, use 20 for min_samples_split to prevent overfitting\n",
    "dtc = DecisionTreeClassifier(random_state=1, min_samples_split=20).fit(X_train, y_train)\n",
    "compute_metrics(dtc, \"Decision Trees\")\n",
    "\n",
    "# Random Forest, with a 100 estimators and use min_samples_split\n",
    "rfc = RandomForestClassifier(n_estimators=100, min_samples_split=20, random_state=1).fit(X_train, y_train)\n",
    "compute_metrics(rfc, \"Random Forest Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=’Eval’></a>\n",
    "# Evaluating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going with the Linear Regression model because it was able to predict ~54% of the viral videos with decent accuracy. I retrain the Linear Regressor on both the training and validation. Seeing some promising results with ~85% for both accuracy and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Refit on Train and Validation\n",
    "X_combined = np.concatenate((X_train, X_validation))\n",
    "y_combined = np.append(y_train, y_validation)\n",
    "\n",
    "lr = LogisticRegression(random_state=1, solver='lbfgs').fit(X_combined, y_combined)\n",
    "compute_metrics(lr, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looks like it fell flat on the Test set with 79.52% accuracy and only 57% recall. Some work to do with tuning the hyper-parameters further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on test\n",
    "predictions = custom_predict(lr, X_test)\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "prec = precision_score(y_test, predictions, pos_label='Viral')\n",
    "rec = recall_score(y_test, predictions, pos_label='Viral')\n",
    "\n",
    "print(f\"Scores from the best model: \\n  Accuracy: {round(acc * 100, 2)}%\\n  Precision: {round(prec * 100, 2)}%\\n  Recall: {round(rec * 100, 2)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
